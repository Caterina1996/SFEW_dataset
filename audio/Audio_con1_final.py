# -*- coding: utf-8 -*-
"""Copia_de_Copia_final_Audio4v2_new_bo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z5eIQ9RP1FRkcLm-b2cesx-Jag9Xumtz

## **Audio Configuració 1 - Log-Mel-Spectrograms and CNN**
"""

from google.colab import drive
drive.mount('/content/gdrive/', force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install ffmpeg

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !unzip "/content/gdrive/My Drive/EmotiW_2018.zip"

import ffmpeg
import glob, os,shutil

import matplotlib.pyplot as plt
import numpy as np
from numpy import asarray
from numpy import save
from numpy.random import seed
import pandas as pd

import librosa                    
import librosa.display
import IPython.display as ipd
import scipy.io.wavfile
from scipy.fftpack import dct
from scipy import signal

import operator
import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential,Model
from keras.layers import Dropout, Flatten, Dense, Conv2D, BatchNormalization, MaxPooling2D,Activation
from keras import applications
from keras.models import load_model
from keras import regularizers
from keras.models import load_model
from keras.callbacks import ModelCheckpoint,CSVLogger
import tensorflow as tf
from keras import regularizers, optimizers

from sklearn.model_selection import train_test_split
import sklearn.metrics as sk_metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

import itertools
import shutil

import csv
from csv import DictWriter
from datetime import datetime
from packaging import version
from pathlib import Path

#Set seed
seed(42)
tf.random.set_seed(42)

"""### **Extract audio from AFEW videos**"""

dict_xy_afew={}
train_set_path='EmotiW_2018/Train_AFEW/'
test_set_path='EmotiW_2018/Val_AFEW/'

os.remove(train_set_path+'Train_6.xml')
os.remove(train_set_path+'AlignedFaces_LBPTOP_Points.zip')
os.remove(test_set_path+'Val_6.xml')
os.remove(test_set_path+'AlignedFaces_LBPTOP_Points_Val.zip')

emotions=sorted(os.listdir(train_set_path))
print(emotions)

num_emotions=7
emo_dict={}
for emo,i in zip(emotions, range(num_emotions+1)):
  emo_dict[emo]=i
print(emo_dict)

emo_dict_reversed = {v: k for k, v in emo_dict.items()}
print(emo_dict_reversed)

#Convert a video set into audio format
def convert_to_audio_ne(set_path,set_name):
  f = open("convert_to_audio.sh", "w")
  path_afew_set=set_path
  urls_afew = glob.glob(pathname=path_afew_set + '*')
  print(urls_afew)
  print("-------------------")
  for url_emo in urls_afew:
    urls_emotion = glob.glob(pathname=url_emo + '/*')
    # print(urls_emotion)
    emo=os.path.basename(url_emo)
    new_dir=set_name+'_audio_ne/'
    if not os.path.exists(set_name+'_audio_ne'):
      os.makedirs(set_name+'_audio_ne')
    print(emo)
    for video_file in urls_emotion:
      file_name=os.path.splitext(os.path.basename(video_file))
      f.write("ffmpeg -i "+video_file+" -vn -ac 1 "+new_dir+file_name[0]+".wav"+'\n')
      dict_xy_afew[file_name[0]]=emo
  f.close()

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# #Extract audio from the videos
# !rm -rf Train_audio_ne
# convert_to_audio_ne(train_set_path,'Train')
# ! bash convert_to_audio.sh
# !rm -rf Test_audio_ne
# convert_to_audio_ne(test_set_path,'Test')
# ! bash convert_to_audio.sh

train_path='Train_audio_ne/'
test_path='Test_audio_ne/'

urls_afew_train = glob.glob(pathname=train_path + '*')
urls_afew_test = glob.glob(pathname=test_path + '*')
urls_afew_test[0]

print("number of train samples:",len(urls_afew_train))
print("number of test samples:",len(urls_afew_test))

emotions=[]
num_samples=[]
for emo in emo_dict.keys():
  samples=list(dict_xy_afew.values()).count(emo)
  num_samples.append(samples)
  emotions.append(emo)
  print("emotion: ", emo)
  print("number of audios: ",samples)
  print("-----")

#Está desbalanceado. Graficamos la distribución por clases:

fig = plt.figure(figsize = (10, 5))
plt.bar(emotions, num_samples, 
        0.35,label="number of samples",color ='blue')
plt.xlabel("emotion classes")
plt.ylabel("No. of samples")
plt.title("Number of samples per emotion in the AFEW dataset")
plt.show()

"""### **Convert audio to Log-Mel_Spectrograms**

**Audio preprocessing**

Auxiliar functions:
"""

#Auxiliar functions:

def get_categorical_label(file_id):
  emo=dict_xy_afew[file_id]
  categorical_label=tf.keras.utils.to_categorical(emo_dict[emo], num_classes=num_classes)
  return categorical_label

def get_emo_label(categorical_label):
  emo=dict_xy_afew[file_id]
  return emo

def get_emo_from_categorical(label):
  idx=list(label).index(1)
  return emo_dict_reversed[idx]

def plot_model(history,title):
  fig=plt.figure()
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  # plt.plot(history.history['val_categorical_accuracy'])
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'val'], loc='upper left')
  plt.title("model train and val accuracy "+title)
  
  plt.figure()
  plt.plot(history.history['val_accuracy'])
  plt.title("Validation accuracy " + title)
  plt.show()

def save_plot(history,title,filename="/content/gdrive/My Drive/TFM_MUSI/Audio/Results/"):
  fig = plt.figure()
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  # plt.plot(history.history['val_categorical_accuracy'])
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'val'], loc='upper left')
  plt.title("model train and val accuracy "+title)
  plt.show()
  new_filename  = Path(filename +'/'+title+'.jpg')
  fig.savefig(new_filename, dpi=400, bbox_inches='tight',pad_inches=0)

#Plots metrics and confusion matrix
def plot_conf_matrix(x_test,y_test,modelo,path,title,standard_format=True,percentages=True,save_plot=True):

  labels=[key for key in emo_dict.keys()]
  predictions=modelo.predict(x_test)

  ypred=[round(list(i).index(np.max(i))) for i in predictions]
  y_true=[np.argmax(y) for y in y_test]

  print(confusion_matrix(y_true, ypred)[:,:])
  print("")
  f1=f1_score(y_true, ypred,average=None)
  f1_w=f1_score(y_true, ypred,average='weighted')
  ac=accuracy_score(y_true, ypred)
  print("accuracy: ",ac)
  print("f1 score:",f1)
  print("f1 score weighted:",f1_w)
  print("---------------------------------------------------------")
  print(title)
  if standard_format:
    cm = confusion_matrix(y_true, ypred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap=plt.cm.Blues,xticks_rotation='vertical',values_format='d')
    if save_plot:
      plt.savefig(path+".png")
    plt.show()
  print("")
  if percentages:
    cm = confusion_matrix(y_true, ypred,normalize='true')
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap=plt.cm.Blues,xticks_rotation='vertical',values_format='.2f')
    plt.show()
    if save_plot:
      plt.savefig(path+"_perctg.png")
    plt.show()
  print("")

def compile_and_fit(model,chkpoint_filepath,csv_log_file,epochs,batch_size,lr=0.001):
  optimizer = tf.keras.optimizers.RMSprop(lr=lr)
  model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy', metrics=['categorical_accuracy','accuracy'])
  
  logdir = chkpoint_filepath + "corrected_test"
  
  if not os.path.exists(logdir):
      os.mkdir(logdir)
  tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)
  
  # filepath="/content/gdrive/My Drive/TFM_MUSI/FACES6/:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5"
  chkpoint_filepath=chkpoint_filepath+":{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5"
  checkpoint = ModelCheckpoint(chkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
  
  # csv_logger = CSVLogger('/content/gdrive/My Drive/TFM_MUSI/training.log')
  csv_logger = CSVLogger(csv_log_file)
  
  E_stopping=tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                              min_delta=0,
                                              patience=30,
                                              verbose=1,
                                              mode="auto",
                                              restore_best_weights=False)

  # callbacks_list = [checkpoint,csv_logger]
  callbacks_list = [E_stopping]
  # callback_list = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=35)
  # history=model.fit(xtrain, Y_train,
  #                  epochs=epochs,
  #                  validation_data=(xtest, Y_test),
  #                  batch_size=batch_size,
  #                  class_weight=class_weight_dict,
  #                  callbacks=callback_list)

  history=model.fit(xtrain, y_train_new,
                   epochs=epochs,
                   validation_data=(xval, y_val_new),
                   batch_size=batch_size,
                  #  class_weight=class_weight_dict,
                   callbacks=callbacks_list)

  return history,model

# get min lenght of audio sets
urls=urls_afew_train[:]
urls.extend(urls_afew_test)
audios_len=[]
max=0
for audio_sample in urls:
  # print(urls_afew_train)
  audio,sr = librosa.load(audio_sample,sr=22050)
  audios_len.append(audio.shape[0])
  if audio.shape[0]>max:
    max=audio.shape[0]
    print("new max",max)
    print(audio_sample)
    print("----------------")

print("maximum audio length: ", np.max(audios_len), "is from audio:",urls[audios_len.index(np.max(audios_len))] )
print("minimum audio length: ",np.min(audios_len), "is from audio:",urls[audios_len.index(np.min(audios_len))])
print("mean audio length : ",np.mean(audios_len))

#Analizamos como son el audio más corto y más largo del dataset:

#Uncomment to listen to the audios:

# #Audio más corto
# audio,sr = librosa.load("Train_audio_ne/011603980.wav")
# librosa.get_duration(audio)
# librosa.display.waveplot(audio)
# print(librosa.get_duration(audio))
# print(sr)
# ipd.Audio("Train_audio_ne/011603980.wav")

# #Audio más largo:
# len(urls)
# print(urls[934])
# audio,sr = librosa.load("Test_audio_ne/003353294.wav")
# librosa.display.waveplot(audio, sr=sr)
# print(librosa.get_duration(audio))
# audio.shape[0]
# print(sr)
# ipd.Audio("Test_audio_ne/003353294.wav")

# #Audio más largo del set de entrenamiento
# len(urls)
# print(urls[934])
# audio,sr = librosa.load("Train_audio_ne/003853320.wav")
# librosa.display.waveplot(audio, sr=sr)
# print(librosa.get_duration(audio))
# audio.shape[0]
# print(sr)
# ipd.Audio("Train_audio_ne/003853320.wav")

"""**Zero-padding to standarize audios lenght**"""
  #resize audios to a new standard size shortening or padding with zeros
def standarize_audios_len(audio_file,desired_len):

  len=asarray(audio_file).shape
  # print(len)
  # print("dif=", abs(len[0]-desired_len))
  if len[0] < desired_len:
    # print("short")
    zeros=np.zeros(abs(len[0]-desired_len))
    new_audio=np.append(audio_file,zeros)
  else:
    new_audio=audio_file[0:desired_len]
  
  return new_audio

"""**Preprocess audio and compute log-mel-spectrogram**"""
  #function to preprocess audio:
  # standarizes the audio lenght to the maximum audio len padding with 0s so all audios have the same dimensions
  # Applies a preemphasis filter if set
  # Computes de log-mel-spectrogram and normalizes it if set

def preprocess_audio(audio_file, desired_audio_len=137063, frame_size=0.025, stride=0.01,normalize=False,preemphasis=True,norm_waveform=False):
  #normalize waveform
  if norm_waveform==True:
    audio_file = (audio_file - audio_file.mean()) / audio_file.std() + 1e-10

  y,sr = librosa.load(audio_file) 
  # shortened_audio=y[0:8997]
  y=standarize_audios_len(y,desired_audio_len) #standarize the len of the audios padding with 0s:

  if preemphasis==True: 
    y=librosa.effects.preemphasis(y, coef=0.97) #Apply a pre-emphasis filter

  #frame size mejor potencia de 2?  
  frame_size=int(round(frame_size*sr)) #frame_size en samples dim
  stride_size= int(round(stride*sr))  #stride_size en samples dim
  mel_spectrogram = librosa.feature.melspectrogram(y, sr=sr, n_fft=frame_size,window=scipy.signal.hamming, hop_length=stride_size, n_mels=40)
  log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)
  
  if normalize==True:
    mean = np.mean(log_mel_spectrogram, axis=0)
    std = np.std(log_mel_spectrogram, axis=0)
    log_mel_spectrogram = (log_mel_spectrogram - mean) / std + 1e-10

  return log_mel_spectrogram,sr

def create_set(url_list, dict_set, desired_audio_len=137063, normalize=False, preemphasis=True):
  x_data,y_data=[],[]
  mean=[]
  spec_values=[]
  i=0
  for audio_file in url_list:
    file_id=os.path.splitext(os.path.basename(audio_file))[0]
    melSpec_dB,sr = preprocess_audio(audio_file,desired_audio_len=137063, frame_size=0.025, stride=0.01,normalize=normalize,preemphasis=preemphasis)
    mean.extend(np.mean(melSpec_dB,axis=0))
    spec_values.extend(melSpec_dB)
    
    #   #check
    #   plt.figure(figsize=(10, 5))
    #   librosa.display.specshow(melSpec_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)
    #   plt.colorbar(format='%+1.0f dB')
    #   plt.title("MelSpectrogram")
    #   plt.tight_layout()
    #   plt.show()

    label=get_categorical_label(file_id)
    dict_set[audio_file]=label
    x_data.append(melSpec_dB)
    y_data.append(label)
  return x_data,y_data,mean,spec_values

"""**Normalization**"""

normalize=False 
preemphasis=True
desired_audio_len=137063 #maximum len
num_classes=7

#Dictionaries containing audio id and label
train_data={}
test_data={}

x_data_train,y_data_train,mean,spec_values = create_set(urls_afew_train, train_data, desired_audio_len ,normalize,preemphasis)

x_data_test,y_data_test,_,_ = create_set(urls_afew_test,test_data, desired_audio_len, normalize,preemphasis)

#not channelized mean and std
#global normalization:

global_tr_mean=np.mean(x_data_train)
global_tr_std=np.std(x_data_train)

print(global_tr_mean)
print(global_tr_std)

global_norm_data_train=[]
for spec in x_data_train:
  norm_spec=(spec-global_tr_mean)/global_tr_std+1e-10
  global_norm_data_train.append(norm_spec) 

print(asarray(global_norm_data_train).shape)
print(global_norm_data_train[0])

global_norm_data_test=[]
for spec in x_data_test:
  norm_spec=(spec-global_tr_mean)/global_tr_std+1e-10
  global_norm_data_test.append(norm_spec) 

print(asarray(global_norm_data_test).shape)
print(global_norm_data_test[0])

#check normalization:
# print(np.mean(global_norm_data_train))
# print(np.std(global_norm_data_train))

# print(np.mean(global_norm_data_test))
# print(np.std(global_norm_data_test))

#calculamos la media del conjunto de entrenamiento por mel band 

xtr=asarray(x_data_train)
print(xtr.shape)

temp_mean=np.mean(xtr,axis=2)
temp_std=np.std(xtr,axis=2)

print(temp_mean.shape)
print(temp_std.shape)
train_mean=np.mean(temp_mean,axis=0)
train_std=np.std(temp_std,axis=0)

print(train_mean.shape)
print(train_std.shape)

train_mean

train_std

new_data_train=[]

for spec in x_data_train:
  norm_spec=(spec.T-train_mean)/train_std+1e-10
  new_data_train.append(norm_spec.T) 

print(asarray(new_data_train).shape)
print(new_data_train[0])

new_data_test=[]
for spec in x_data_test:
  norm_spec=(spec.T-train_mean)/train_std+1e-10
  new_data_test.append(norm_spec.T)

print(asarray(new_data_test).shape)
print(new_data_test[0])

#PARAMS:
DROPOUT=0.5
l2_regul=0.01
N_OUTPUT_LABELS=7
input_shape=asarray(new_data_train[0]).shape
# input_shape=asarray(x_data_train[0]).shape
input_shape=list(input_shape)
input_shape.append(1)
print("input shape: ",input_shape)
epochs=50
batch_size=16
lr=0.0001

"""### **Model 0 (UrbandSound)**

**Figura 14:** Modelo del audio 0 basado en el modelo del UrbandSound
"""

#UrbandSoundmodel (model 0)
from keras.utils.vis_utils import plot_model
def model1_US():
  model = Sequential()
  model.add(Conv2D(32, (3, 3), padding='same',
                  input_shape=input_shape
                  ))
  model.add(Activation('relu'))
  model.add(Conv2D(64, (3, 3)))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.25))
  model.add(Conv2D(64, (3, 3), padding='same'))
  model.add(Activation('relu'))
  model.add(Conv2D(64, (3, 3)))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.5))
  model.add(Conv2D(128, (3, 3), padding='same'))
  model.add(Activation('relu'))
  model.add(Conv2D(128, (3, 3)))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.5))
  model.add(Flatten())
  model.add(Dense(512))
  model.add(Activation('relu'))
  model.add(Dropout(0.5))
  model.add(Dense(7, activation='softmax'))
  return model
model=model1_US()
# model.compile(optimizers.RMSprop(lr=0.0005, decay=1e-6),loss="categorical_crossentropy",metrics=["accuracy"])
model.summary()
plot_model(model,to_file='audio_model0_US_plot.png', show_shapes=True, show_layer_names=True)

#Test model:

#Test amb el model de l'urbandsound
model=model1_US()
# model.compile(optimizers.RMSprop(lr=0.0005, decay=1e-6),loss="categorical_crossentropy",metrics=["accuracy"])

history,model=compile_and_fit(model,chkpoint_filepath,csv_log_file,epochs,batch_size,lr=lr)
model.save("/content/gdrive/My Drive/TFM_MUSI/Audio/Prueba4v2/US_spec_norm.h5")
plot_model(history,"P4v2 model US max len norm")

#TEST MODELO 0 (MODELO URBANDSOUND): 

# X_tr_arr=asarray(new_data_train)
y_train_arr=asarray(y_data_train)
# X_te_arr=asarray(new_data_test)
y_te_arr=asarray(y_data_test)

# xtrain = tf.expand_dims(X_tr_arr, axis=-1)
# xtest= tf.expand_dims(X_te_arr, axis=-1)

epochs=50
batch_size=16
lr=0.0001

lr_rates=[0.0001,0.00005,0.00001]
batch_szs=[8,16,32,64]
norm=[0,1,2]

for lr in lr_rates:
  for batch_size in batch_szs:
    for normalized in norm:
      print("-------------------------------------")
      print("learnig rate: ",lr)
      print("batch size: ",batch_size)
      print("normalized?: ",norm)

      if normalized==0:
        X_tr_arr=asarray(x_data_train)
        X_te_arr=asarray(x_data_test)
        xtrain = tf.expand_dims(X_tr_arr, axis=-1)
        xtest= tf.expand_dims(X_te_arr, axis=-1)
        norm_str="mel band normalized"
      elif normalized==1:
        X_tr_arr=asarray(new_data_train)
        X_te_arr=asarray(new_data_test)
        xtrain = tf.expand_dims(X_tr_arr, axis=-1)
        xtest= tf.expand_dims(X_te_arr, axis=-1)
        norm_str="not normalized"
        # norm_str="normalized"
      elif normalized==2:
        X_tr_arr=asarray(global_norm_data_train)
        X_te_arr=asarray(global_norm_data_test)
        xtrain = tf.expand_dims(X_tr_arr, axis=-1)
        xtest= tf.expand_dims(X_te_arr, axis=-1)
        norm_str="global normalization"

      print("normalization type: ",norm_str)

      model_US=model1_US()
      optimizer = tf.keras.optimizers.RMSprop(lr=lr,decay=1e-6)
      model_US.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['categorical_accuracy','accuracy'])

      E_stopping=tf.keras.callbacks.EarlyStopping(monitor="val_loss",min_delta=0,patience=15,verbose=1,mode="auto",restore_best_weights=True)
      callbacks_list=[E_stopping]

      history=model_US.fit(xtrain, y_train_arr,
                        epochs=epochs,
                        validation_data=(xtest, y_te_arr),
                        batch_size=batch_size,
                      #  class_weight=class_weight_dict,
                        callbacks=callbacks_list)
      path=""
      title="Audio model 1(US) trained with log-mel-specs"+"lr= "+str(lr)+" batch_sz="+str(batch_size)+" "+norm_str
      plot_model(history,title)
      plot_conf_matrix(xtest,y_te_arr,model_US,path,title,standard_format=True,percentages=True,save_plot=True)
      plt.figure()

"""### **MODELO 1 (Fayek)**

**Figura 15:** Modelo 1 del audio inspirado en el trabajo de Fayek, Lech y Cavedon
"""

#Fayek model (model 2)
def Classifier():
    model = Sequential()

    model.add(Conv2D(filters=16,
                     kernel_size=(10, 10),
                     strides=2,
                     activation='relu',
                     input_shape=input_shape, kernel_regularizer=regularizers.l2(l2_regul)))
    
    model.add(BatchNormalization())
    # model.add(Dropout(DROPOUT))


    model.add(Conv2D(filters=32,
                     kernel_size=(10, 10),
                     strides=2,
                     activation='relu',
                     kernel_regularizer=regularizers.l2(l2_regul)))
    
   
    model.add(BatchNormalization())
    # model.add(Dropout(DROPOUT))

    #Afegit això:
    # model.add(MaxPooling2D(pool_size = (2, 2)))
    model.add(Flatten())
    
    model.add(Dense(units=716,activation='relu'))
    
    model.add(BatchNormalization())
    model.add(Dropout(DROPOUT))

    model.add(Dense(units=716,activation='relu'))

    model.add(BatchNormalization())
    model.add(Dropout(DROPOUT))

    model.add(Dense(units=716,activation='softmax'))


    model.add(BatchNormalization())
    model.add(Dropout(DROPOUT))
    # model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Dense(units=N_OUTPUT_LABELS,
                    activation='softmax'))
    return model


audio_model = Classifier()
audio_model.summary()
from keras.utils.vis_utils import plot_model
plot_model(audio_model,to_file='audio_model1_plot.png', show_shapes=True, show_layer_names=True)

#Test params
y_train_arr=asarray(y_data_train)
y_te_arr=asarray(y_data_test)

epochs=50
batch_size=16
lr=0.0001

lr_rates=[0.0001,0.00005,0.00001]
batch_szs=[8,16,32,64]
norm=[0,1]

for lr in lr_rates:
  for batch_size in batch_szs:
    for normalized in norm:
      print("-------------------------------------")
      print("learnig rate: ",lr)
      print("batch size: ",batch_size)
      print("normalized audio?: ",normalized==1)
      if normalized==1:
        X_tr_arr=asarray(new_data_train)
        X_te_arr=asarray(new_data_test)
        xtrain = tf.expand_dims(X_tr_arr, axis=-1)
        xtest= tf.expand_dims(X_te_arr, axis=-1)
        norm_str="normalized"
      else:
        X_tr_arr=asarray(x_data_train)
        X_te_arr=asarray(x_data_test)
        xtrain = tf.expand_dims(X_tr_arr, axis=-1)
        xtest= tf.expand_dims(X_te_arr, axis=-1)
        norm_str="not normalized"

      audio_model2_fayek = Classifier()
      optimizer = tf.keras.optimizers.RMSprop(lr=lr)
      audio_model2_fayek.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['categorical_accuracy','accuracy'])

      E_stopping=tf.keras.callbacks.EarlyStopping(monitor="val_loss",min_delta=0,patience=20,verbose=1,mode="auto",restore_best_weights=True)
      callbacks_list=[E_stopping]

      history=audio_model2_fayek.fit(xtrain, y_train_arr,
                        epochs=epochs,
                        validation_data=(xtest, y_te_arr),
                        batch_size=batch_size,
                      #  class_weight=class_weight_dict,
                        callbacks=callbacks_list)
      path=""
      title="Audio model 2(Fayek) trained with log-mel-specs"+"lr= "+str(lr)+" batch_sz="+str(batch_size)+" "+norm_str
      plot_model(history,title)
      plot_conf_matrix(xtest,y_te_arr,audio_model2_fayek,path,title,standard_format=True,percentages=True,save_plot=True)
      plt.figure()

