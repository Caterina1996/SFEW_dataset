{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqz34H2KbBJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5f28f0-1c18-43f6-ca6c-ba7017213f44"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-MxWZfQa0UQ",
        "outputId": "d5c27568-a337-482f-9ea3-20a91aa97272"
      },
      "source": [
        "#Load SFEW data:\n",
        "!rm -rf SFEW_dataset\n",
        "\n",
        "!git clone https://github.com/Caterina1996/SFEW_dataset\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SFEW_dataset'...\n",
            "remote: Enumerating objects: 1345, done.\u001b[K\n",
            "remote: Counting objects: 100% (1345/1345), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1332/1332), done.\u001b[K\n",
            "remote: Total 1345 (delta 18), reused 1328 (delta 13), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1345/1345), 41.10 MiB | 29.27 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuSTVelHOUPV"
      },
      "source": [
        "%%capture\n",
        "! unzip /content/gdrive/MyDrive/EmotiW_2018.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPQyuTPXfb6C"
      },
      "source": [
        "#Unzip faces frames and change folder names\n",
        "%%capture \n",
        "! unzip /content/EmotiW_2018/Val_AFEW/AlignedFaces_LBPTOP_Points_Val.zip\n",
        "!mv /content/AlignedFaces_LBPTOP_Points_Val /content/Val_faces_videos\n",
        "\n",
        "\n",
        "!unzip /content/EmotiW_2018/Train_AFEW/AlignedFaces_LBPTOP_Points.zip\n",
        "!mv AlignedFaces_LBPTOP_Points Train_faces_videos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a25qmDMnbVen"
      },
      "source": [
        "import operator\n",
        "import keras\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense, Conv2D, BatchNormalization, MaxPooling2D\n",
        "from keras import applications\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "import sklearn.metrics as sk_metrics\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "import glob\n",
        "import ntpath as nt\n",
        "import os\n",
        "from PIL import Image\n",
        "import operator\n",
        "from keras import optimizers\n",
        "from shutil import copyfile\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from keras import applications\n",
        "import copy\n",
        "import collections\n",
        "import pickle\n",
        "# https://riptutorial.com/keras/example/32608/transfer-learning-using-keras-and-vgg\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "\n",
        "#fix weird vgg problem:\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7-DB3ah9t9c"
      },
      "source": [
        "#Set seed\n",
        "seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcS8ErlvbZB4"
      },
      "source": [
        "#Parameters initializations:\n",
        "\n",
        "img_width, img_height =128, 128 #revisar Las originales son 143 x 181\n",
        "batch_size=2\n",
        "learning_rate = 0.00001\n",
        "epochs = 100\n",
        "batch_size = 2\n",
        "architecture='VGG16'\n",
        "img_width, img_height=img_width, img_height\n",
        "num_classes=7\n",
        "dropout = 0.25\n",
        "dataugmentation=True\n",
        "\n",
        "train_dir='SFEW_dataset/Train'\n",
        "test_dir='SFEW_dataset/Val'\n",
        "\n",
        "emotions_list=os.listdir(\"/content/SFEW_dataset/Train\")\n",
        "inidices=list(range(0,7))\n",
        "\n",
        "emotions_dict=dict(zip(inidices,emotions_list))\n",
        "emotions_reversed_dict = {v: k for k, v in emotions_dict.items() }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OOr3SYBJPq4",
        "outputId": "41503b1c-8298-447d-9424-1464bb3e92ff"
      },
      "source": [
        "emotions_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Fear',\n",
              " 1: 'Sad',\n",
              " 2: 'Surprise',\n",
              " 3: 'Happy',\n",
              " 4: 'Neutral',\n",
              " 5: 'Disgust',\n",
              " 6: 'Angry'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqlsOAlrhA17"
      },
      "source": [
        "def create_labels_set_dir(set_path):\n",
        "  video_labels={}\n",
        "  for emotion in emotions_list:\n",
        "    for video_id in os.listdir(set_path+emotion):\n",
        "      video_labels[video_id.split(\".\")[0]]=emotion\n",
        "  return video_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GiHXePIYb82",
        "outputId": "fca98be8-4d1a-4a91-cf72-dc5462c8ae6d"
      },
      "source": [
        "def get_emo(prediction):\n",
        "  emo=list(prediction).index(max(prediction))\n",
        "  return emotions_dict[emo]\n",
        "\n",
        "def get_emo(prediction):\n",
        "  emo=list(prediction).index(1)\n",
        "  return emotions_dict[emo]\n",
        "\n",
        "def get_emo_value(prediction):\n",
        "  return list(prediction).index(max(prediction))\n",
        "\n",
        "emotions_dict\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Fear',\n",
              " 1: 'Sad',\n",
              " 2: 'Surprise',\n",
              " 3: 'Happy',\n",
              " 4: 'Neutral',\n",
              " 5: 'Disgust',\n",
              " 6: 'Angry'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G39vpaF-hZvr"
      },
      "source": [
        "val_dir=\"/content/EmotiW_2018/Val_AFEW/\"\n",
        "val_video_labels=create_labels_set_dir(val_dir)\n",
        "\n",
        "train_dir=\"/content/EmotiW_2018/Train_AFEW/\"\n",
        "train_video_labels=create_labels_set_dir(train_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPAJ5-oUYgrt",
        "outputId": "d301f25f-a9df-4560-aecd-ce635a27d207"
      },
      "source": [
        "print(len(val_video_labels))\n",
        "print(len(train_video_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "381\n",
            "773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMup0Y2XGQ74"
      },
      "source": [
        "#save afew labels\n",
        "a_file = open(\"/content/gdrive/MyDrive/TFM_MUSI/combined_model/train_video_labels.pkl\", \"wb\")\n",
        "\n",
        "pickle.dump(train_video_labels, a_file)\n",
        "\n",
        "a_file.close()\n",
        "\n",
        "a_file = open(\"/content/gdrive/MyDrive/TFM_MUSI/combined_model/val_video_labels.pkl\", \"wb\")\n",
        "\n",
        "pickle.dump(val_video_labels, a_file)\n",
        "\n",
        "a_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H--At_HcGN4O"
      },
      "source": [
        "for emotion in emotions_list:\n",
        "  if not os.path.exists(\"SFEW_images_videos/Train/\"+emotion):\n",
        "    os.makedirs(\"SFEW_images_videos/Train/\"+emotion)\n",
        "\n",
        "  if not os.path.exists(\"EMOTIW_test_Faces/\"+emotion):\n",
        "    os.makedirs(\"SFEW_images_videos/Val/\"+emotion)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsoNNQOC2ECD"
      },
      "source": [
        "#group sfew images acording to their video label\n",
        "def tidy_SFEW(origin_dir,destination_dir,emo_video_labels):\n",
        "\n",
        "  for emo in emotions_list:\n",
        "    if not os.path.exists(destination_dir+emo):\n",
        "      print(\"HI\")\n",
        "      os.makedirs(destination_dir+emo)\n",
        "\n",
        "  # obtener la emocion del video a la que pertenecen las imagenes del sfew\n",
        "  val_images=glob.glob(origin_dir+\"*/*/*\")\n",
        "  val_images\n",
        "  i=0\n",
        "  print(\"dir0\",val_images[0])\n",
        "  for image_path in val_images:\n",
        "    print(\"image path:\",image_path)\n",
        "\n",
        "    image_video_id=image_path.split(\"/\")[-1].split(\"_\")[-2]\n",
        "    image_id=image_path.split(\"/\")[-1].split(\"_\")[-1]\n",
        "    sfew_emo=image_path.split(\"/\")[-2]\n",
        "\n",
        "    print(\"video_id:\",image_video_id)\n",
        "    print(\"image_id:\",image_id)\n",
        "    print(\"sfew_emo:\",sfew_emo)\n",
        "\n",
        "    try:\n",
        "      afew_emo=emo_video_labels[image_video_id]\n",
        "      destination=destination_dir+afew_emo+\"/\"+image_video_id\n",
        "      print(\"destination_dir: \",destination)\n",
        "\n",
        "      if not os.path.exists(destination):\n",
        "        os.makedirs(destination)\n",
        "      copyfile(image_path,destination+\"/\"+image_id)\n",
        "      if sfew_emo != afew_emo:\n",
        "        print(\"SFEW emo and afew emo dont match\")\n",
        "        print(\"SFEW emo was :\", sfew_emo)\n",
        "        print(\"AFEW emo is :\", afew_emo)\n",
        "        # display \n",
        "        im = Image.open(image_path)\n",
        "        plt.imshow(im)\n",
        "        plt.show()\n",
        "        i+=1\n",
        "        print(\"number of discrepances is\", i)\n",
        "\n",
        "    except Exception as e: \n",
        "      print(e)\n",
        "      print(\"id not found: \",image_video_id)\n",
        "\n",
        "print(\"TRAIN---------------------------------------------------------------------------\")\n",
        "origin_dir=\"/content/SFEW_dataset/Train\"\n",
        "destination_dir=\"/content/SFEW_images_videos/Train/\"\n",
        "tidy_SFEW(origin_dir,destination_dir,train_video_labels)\n",
        "\n",
        "print(\"TEST---------------------------------------------------------------------------\")\n",
        "origin_dir=\"/content/SFEW_dataset/Val\"\n",
        "destination_dir=\"/content/SFEW_images_videos/Val/\"\n",
        "tidy_SFEW(origin_dir,destination_dir,val_video_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQueXto-TDG1"
      },
      "source": [
        "#Load static model Vgg16 with last block retrained with SFEW\n",
        "#We will use this model to predict frame emotions of the video\n",
        "model=keras.models.load_model(\"/content/gdrive/MyDrive/TFM_MUSI/VGGmodel/SFEW_corrected/SFEW_TF_corrected0.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZGP_eJCLof9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d629afb-d242-41dd-8d7a-db1223351bd9"
      },
      "source": [
        "#Analize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              8389632   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 7175      \n",
            "=================================================================\n",
            "Total params: 23,111,495\n",
            "Trainable params: 15,476,231\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpq2mxBAjjVj"
      },
      "source": [
        "#Create sets\n",
        "\n",
        "\n",
        "train_data_dir='SFEW_dataset/Train'\n",
        "test_data_dir='SFEW_dataset/Val'\n",
        "\n",
        "# train_set,test_set=create_sets(dataugmentation,train_dir,test_dir,batch_size)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                   brightness_range=[0.5,1.0],\n",
        "                                   zoom_range=0.1,\n",
        "                                   shear_range=0.2,\n",
        "                                  #  width_shift_range=[-40,40]\n",
        "                                  #  height_shift_range=0.1,\n",
        "                                   validation_split=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "                                  #  height_shift_range=0.1\n",
        "\n",
        "test_datagen= ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    # color_mode='grayscale',\n",
        "    subset='training',\n",
        "    shuffle=True)\n",
        "\n",
        "# val_set = train_datagen.flow_from_directory(\n",
        "#     train_data_dir, # same directory as training data\n",
        "#     target_size=(img_height, img_width),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical',\n",
        "#     subset='validation') \n",
        "  \n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    # color_mode='grayscale',\n",
        "    shuffle=True)\n",
        "\n",
        "nb_train_samples = len(train_set.filenames)\n",
        "nb_test_samples = len(test_set.filenames)\n",
        "# nb_val_samples = len(val_set.filenames)\n",
        "\n",
        "train_set.class_indices\n",
        "emotions_dict = {v: k for k, v in train_set.class_indices.items()}\n",
        "print(emotions_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGx9_1WNK6Qr"
      },
      "source": [
        "#For each video compute predictions of video frames using the tuned vgg16 model\n",
        "#already trained on static imgs and average the video frames predictions to obtain\n",
        "#a video predictions vector\n",
        "\n",
        "#Saves video predictions in a dictionary where keys are video labels and values are video predictions\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "def evaluate_videos_score(videos_paths,model):\n",
        "  # videos_paths=glob.glob(\"/content/SFEW_images_videos/*/*\")\n",
        "  y_real=[]\n",
        "  y_pred=[]\n",
        "  predictions=[]\n",
        "  set_preds_dict={}\n",
        "  j=0\n",
        "\n",
        "  for video_path in videos_paths:\n",
        "    print(video_path)\n",
        "    prediction=np.zeros(7)\n",
        "    video_images=glob.glob(video_path+\"/*\")\n",
        "    video_id=video_path.split(\"/\")[-1]\n",
        "    # print(video_images)\n",
        "    n=len(video_images)\n",
        "    print(\"n is: \",n)\n",
        "    i=1\n",
        "\n",
        "    for image_path in video_images:\n",
        "      img=tf.keras.preprocessing.image.load_img(image_path, color_mode=\"rgb\",target_size=(128,128))\n",
        "      x = img_to_array(img)\n",
        "      x=x/255.\n",
        "      # new_x=tf.expand_dims(x, axis=0)\n",
        "      input = np.array([x]) \n",
        "      y=model.predict(input)\n",
        "      # print(y)\n",
        "      \n",
        "      prediction=prediction+y\n",
        "      # print(\"i is: \",i)\n",
        "      # print(\"n is: \",n)\n",
        "      try:\n",
        "        if i==n:\n",
        "          print(\"video num :\",j)\n",
        "          j+=1\n",
        "          prediction=prediction/n  \n",
        "          predictions.append(prediction)\n",
        "          set_preds_dict[video_id]=prediction\n",
        "\n",
        "          print(\"img_path is:\",image_path)\n",
        "          # real_label=image_path.split(\"/\")[4]\n",
        "          real_label=image_path.split(\"/\")[3]\n",
        "          print(\"real label is: \",real_label)\n",
        "          pred_label=get_emo_value(prediction[0])\n",
        "          y_real.append(train_set.class_indices[real_label])\n",
        "          y_pred.append(pred_label)\n",
        "\n",
        "          if real_label!=get_emo(prediction[0]):\n",
        "            print(\"labels don't match!!!!!!----------------------------------------------------\")\n",
        "            print(\"Real label:\",real_label)\n",
        "            print(\"Predicted emo:\",get_emo(prediction[0]))\n",
        "            print(emotions_dict)\n",
        "            print(\"prediction from video:\",prediction[0])\n",
        "\n",
        "            # im = Image.open(image_path)\n",
        "            # plt.imshow(im)\n",
        "            # plt.show()\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "      i+=1\n",
        "\n",
        "  print(confusion_matrix(y_real,y_pred))\n",
        "  print(accuracy_score(y_real, y_pred))\n",
        "  return y_real,y_pred,predictions,set_preds_dict\n",
        "\n",
        "\n",
        "# videos_paths=glob.glob(\"/content/SFEW_images_videos/Train/*/*\")\n",
        "# y_real_sfew_tr,y_pred_sfew_tr=evaluate_videos_score(videos_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS6a01-iaLWZ"
      },
      "source": [
        "#compute predictions\n",
        "videos_paths=glob.glob(\"/content/SFEW_images_videos/Train/*/*\")\n",
        "y_real_sfew_tr,y_pred_sfew_tr,predictions_tr,sfew_predictions_tr=evaluate_videos_score(videos_paths,model)\n",
        "\n",
        "videos_paths=glob.glob(\"/content/SFEW_images_videos/Val/*/*\")\n",
        "y_real_sfew_te,y_pred_sfew_te,predictions_te,sfew_predictions_te=evaluate_videos_score(videos_paths,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-oy7eTqSzX7"
      },
      "source": [
        "#Save predictions\n",
        "import pickle\n",
        "\n",
        "a_file = open(\"/content/gdrive/MyDrive/TFM_MUSI/combined_model/sfew_predictions_tr.pkl\", \"wb\")\n",
        "\n",
        "pickle.dump(sfew_predictions_tr, a_file)\n",
        "\n",
        "a_file.close()\n",
        "\n",
        "a_file = open(\"/content/gdrive/MyDrive/TFM_MUSI/combined_model/sfew_predictions_te.pkl\", \"wb\")\n",
        "\n",
        "pickle.dump(sfew_predictions_tr, a_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nVMPK2BzSPB"
      },
      "source": [
        "#Check that it is saved correctly\n",
        "a_file = open(\"/content/gdrive/MyDrive/TFM_MUSI/combined_model/sfew_predictions_tr.pkl\", \"rb\")\n",
        "\n",
        "output = pickle.load(a_file)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LTwod8mXKcc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOt5Fx4_yKl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwkG_GAQyKqp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGD1SbFrXK1t"
      },
      "source": [
        "#Test con imgs frames del afew (Discarded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzcOdPm3iVv_"
      },
      "source": [
        "#Create new sorted by label folders where we store video frames faces according to their video emotion label:\n",
        "!rm -rf EMOTIW_val_Faces\n",
        "!rm -rf EMOTIW_train_Faces\n",
        "!rm -rf EMOTIW_test_Faces\n",
        "\n",
        "!mkdir EMOTIW_train_Faces\n",
        "!mkdir EMOTIW_val_Faces\n",
        "!mkdir EMOTIW_test_Faces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PatWFGggiQS0"
      },
      "source": [
        "for emotion in emotions_list:\n",
        "  if not os.path.exists(\"EMOTIW_train_Faces/\"+emotion):\n",
        "    os.makedirs(\"EMOTIW_train_Faces/\"+emotion)\n",
        "\n",
        "  if not os.path.exists(\"EMOTIW_val_Faces/\"+emotion):\n",
        "    os.makedirs(\"EMOTIW_val_Faces/\"+emotion)\n",
        "\n",
        "  if not os.path.exists(\"EMOTIW_test_Faces/\"+emotion):\n",
        "    os.makedirs(\"EMOTIW_test_Faces/\"+emotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oxGm_2Gim0H"
      },
      "source": [
        "train_data_dir='SFEW_dataset/Train'\n",
        "test_data_dir='SFEW_dataset/Val'\n",
        "\n",
        "# train_set,test_set=create_sets(dataugmentation,train_dir,test_dir,batch_size)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                   brightness_range=[0.5,1.0],\n",
        "                                   zoom_range=0.1,\n",
        "                                   shear_range=0.2,\n",
        "                                  #  width_shift_range=[-40,40]\n",
        "                                  #  height_shift_range=0.1,\n",
        "                                   validation_split=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "                                  #  height_shift_range=0.1\n",
        "\n",
        "test_datagen= ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    # color_mode='grayscale',\n",
        "    subset='training',\n",
        "    shuffle=True)\n",
        "\n",
        "val_set = train_datagen.flow_from_directory(\n",
        "    train_data_dir, # same directory as training data\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') \n",
        "  \n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    # color_mode='grayscale',\n",
        "    shuffle=True)\n",
        "\n",
        "nb_train_samples = len(train_set.filenames)\n",
        "nb_test_samples = len(test_set.filenames)\n",
        "nb_val_samples = len(val_set.filenames)\n",
        "\n",
        "train_set.class_indices\n",
        "emotions_dict = {v: k for k, v in train_set.class_indices.items()}\n",
        "print(emotions_dict)\n",
        "\n",
        "predictions=evaluate_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY9_DxXKlTRQ"
      },
      "source": [
        "#tidy aligned faces from afew in train and test sets\n",
        "def tidy_set(origin_faces_paths,dest_dir,dict_set,version1=False,videos_val_dir=\"/content/EMOTIW_val_Faces/\",test=False):\n",
        "  i=0\n",
        "  j=0\n",
        "  j_fotos,k_fotos=0,0\n",
        "  k=0\n",
        "  destination_dir=dest_dir\n",
        "\n",
        "  for dir in origin_faces_paths:\n",
        "    video_id=dir.split(\"/\")[-1]\n",
        "    try:\n",
        "      emotion=dict_set[video_id]\n",
        "      foto_files=glob.glob(dir+\"/*\")\n",
        "      if test==False:\n",
        "        #Train or val set\n",
        "        if video_id in videos_val:\n",
        "          print(\"j:\",j)\n",
        "          j_fotos=j_fotos+len(foto_files)\n",
        "          print(\"valfotos: \",j_fotos)\n",
        "          destination_dir=videos_val_dir\n",
        "          j+=1\n",
        "        elif video_id in videos_tr:\n",
        "          print(\"k:\",k)\n",
        "          k_fotos=k_fotos+len(foto_files)\n",
        "          destination_dir=dest_dir\n",
        "          print(\"trainfotos: \",k_fotos)\n",
        "          k+=1\n",
        "     \n",
        "      \n",
        "      print(\"destination dir\", destination_dir)\n",
        "      for foto_file in foto_files:\n",
        "        foto_id=foto_file.split(\"/\")[-1]\n",
        "\n",
        "        if version1==True:\n",
        "          if not os.path.exists(destination_dir+emotion+\"/\"+video_id):\n",
        "            os.makedirs(destination_dir+emotion+\"/\"+video_id)\n",
        "          copyfile(foto_file,destination_dir+emotion+\"/\"+video_id+\"/\"+foto_id)\n",
        "        else:\n",
        "          copyfile(foto_file,destination_dir+emotion+\"/\"+video_id+\"_\"+foto_id)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(\"label error with video id: \",video_id)\n",
        "      print(\"i:\",i)\n",
        "      i=i+1\n",
        "\n",
        "# train set:\n",
        "print(\"TRAINING SET: \")\n",
        "origin_faces_paths=glob.glob(\"/content/Train_faces_videos/Faces/*\")\n",
        "destination_dir=\"/content/EMOTIW_train_Faces/\"\n",
        "tidy_set(origin_faces_paths,destination_dir,train_video_labels,version1=True)\n",
        "\n",
        "#test set:\n",
        "print(\"---------------------------------------------------------\")\n",
        "print(\"VAL SET: \")\n",
        "origin_faces_paths=glob.glob(\"/content/Val_faces_videos/Faces/*\")\n",
        "# origin_faces_paths=glob.glob(\"/content/Faces_val_videos/*\")\n",
        "destination_dir=\"/content/EMOTIW_test_Faces/\"\n",
        "tidy_set(origin_faces_paths,destination_dir,val_video_labels,version1=True,test=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdXN5nYak0Tv"
      },
      "source": [
        "def evaluate_model(model):\n",
        "  datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "  test_set = datagen.flow_from_directory(\n",
        "      '/content/EMOTIW_test_Faces',\n",
        "      target_size=(img_width, img_height),\n",
        "      batch_size=128,\n",
        "      class_mode='categorical',\n",
        "      shuffle=False)\n",
        "\n",
        "  predictions=model.predict(test_set)\n",
        "  print(test_set.class_indices.keys())\n",
        "  print('Confusion Matrix')\n",
        "  ypred=[round(list(i).index(max(i))) for i in predictions]\n",
        "  print(confusion_matrix(test_set.classes, ypred)[:,:])\n",
        "  # print(ypred)\n",
        "  print(\"accuracy: \",accuracy_score(test_set.classes, ypred))\n",
        "  for i in range(0,6):\n",
        "    # print(\"Train: \",list(train_set.class_indices.keys())[i],\" \",list(train_set.labels).count(i))\n",
        "    print(\"Test: \",list(test_set.class_indices.keys())[i],\" \",list(test_set.labels).count(i))\n",
        "\n",
        "  print(\"evaluate: \",model.evaluate_generator(test_set))\n",
        "  print(test_set.class_indices)\n",
        "  x,y = test_set.next()\n",
        "  print(len(x))\n",
        "  import random\n",
        "  for i in range(15):\n",
        "      j=random.randint(1,127)\n",
        "      image = x[j]\n",
        "      plt.imshow( tf.squeeze(image),cmap='gray')\n",
        "      print(\"real\",get_emo(y[j]))\n",
        "      # print(ypred[i])\n",
        "      print(\"predicted:\",emotions_dict[ypred[i]])\n",
        "      plt.show()\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy8yqtYJk7yW"
      },
      "source": [
        "# train set:\n",
        "print(\"TRAINING SET: \")\n",
        "origin_faces_paths=glob.glob(\"/content/Train_faces_videos/Faces/*\")\n",
        "destination_dir=\"/content/EMOTIW_train_Faces/\"\n",
        "tidy_set(origin_faces_paths,destination_dir,train_video_labels,version1=True)\n",
        "\n",
        "#test set:\n",
        "print(\"---------------------------------------------------------\")\n",
        "print(\"VAL SET: \")\n",
        "# origin_faces_paths=glob.glob(\"/content/Val_faces_videos/Faces/*\")\n",
        "origin_faces_paths=glob.glob(\"/content/Faces_val_videos/*\")\n",
        "destination_dir=\"/content/EMOTIW_val_Faces/\"\n",
        "tidy_set(origin_faces_paths,destination_dir,val_video_labels,version1=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZvq42wNlJR8"
      },
      "source": [
        "videos_paths=glob.glob(\"/content/EMOTIW_train_Faces/*/*\")\n",
        "y_real_afew_tr, y_pred_afew_tr, predictions_tr, afew_predictions_tr =evaluate_videos_score(videos_paths,model)\n",
        "\n",
        "videos_paths=glob.glob(\"/content/EMOTIW_val_Faces/*/*\")\n",
        "y_real_afew_val, y_predafew_val, predictions_val, afew_predictions_val=evaluate_videos_score(videos_paths,model)\n",
        "\n",
        "videos_paths=glob.glob(\"/content/EMOTIW_test_Faces/*/*\")\n",
        "y_real_afew_te, y_predafew_te, predictions_te, afew_predictions_te=evaluate_videos_score(videos_paths,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtViw8K5NbZh"
      },
      "source": [
        "import pickle\n",
        "\n",
        "a_file = open(\"/content/gdrive/MyDrive/TFM_MUSI/combined_model/afew_videos_imgs_predictions_m2_tr.pkl\", \"wb\")\n",
        "\n",
        "pickle.dump(afew_predictions_tr, a_file)\n",
        "\n",
        "a_file.close()\n",
        "\n",
        "a_file = open(\"/content/gdrive/MyDrive/TFM_MUSI/combined_model/afew_videos_imgs_predictions_m2_te.pkl\", \"wb\")\n",
        "\n",
        "pickle.dump(afew_predictions_te, a_file)\n",
        "\n",
        "\n",
        "a_file = open(\"/content/gdrive/MyDrive/TFM_MUSI/combined_model/afew_videos_imgs_predictions_m2_val.pkl\", \"wb\")\n",
        "\n",
        "pickle.dump(afew_predictions_val, a_file)\n",
        "\n",
        "a_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxLmvoznQKze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTCKudFJQK9m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}