# -*- coding: utf-8 -*-
"""combined_model_vf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eKSolM9cn1gPyREkk_n8MXu3AGyU3MsQ

<a href="https://colab.research.google.com/github/Caterina1996/SFEW_dataset/blob/TFM_entrega/combinedmodel0_2set_vf.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## **Combined model**

## **Initializations**
"""

from google.colab import drive
drive.mount('/content/gdrive/', force_remount=True)

import glob, os,shutil

import matplotlib.pyplot as plt
import numpy as np
from numpy import asarray
from numpy import save
from numpy.random import seed
from numpy import asarray

import librosa                    
import librosa.display
import IPython.display as ipd
import scipy.io.wavfile
from scipy.fftpack import dct

import tensorflow as tf
from tensorflow.random import set_seed

import operator
import keras
from keras.models import Sequential
from keras.layers import Dropout, Flatten, Dense, Conv2D, BatchNormalization, MaxPooling2D
from keras import applications
from keras.models import load_model
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras import applications
from keras.callbacks import ModelCheckpoint,CSVLogger
from keras import regularizers

from sklearn.metrics import confusion_matrix
import itertools

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn import svm
from sklearn.preprocessing import StandardScaler
import sklearn.metrics as sk_metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

import pickle 
import pandas as pd
import csv

from csv import writer
from csv import DictWriter
from sklearn.model_selection import GridSearchCV
# from tensorflow.keras import layers

#Set seed
seed(42)
tf.random.set_seed(42)

N_OUTPUT_LABELS=7
lr=0.001
batch_size=8

emotions_dict={
 0: 'Angry',
 1: 'Disgust',
 2: 'Fear',
 3: 'Happy',
 4: 'Neutral',
 5: 'Sad',
 6: 'Surprise'}

emotions_reversed_dict = {v: k for k, v in emotions_dict.items() }

def get_emo(prediction):
  emo=list(prediction).index(max(prediction))
  return emotions_dict[emo]

def get_emo_value(prediction):
  return list(prediction).index(max(prediction))

def get_one_hot_pred(prediction):
  one_hot_label=[0]*7
  emo=list(prediction).index(max(prediction))
  one_hot_label[emo]=1
  return one_hot_label

emotions_reversed_dict

labels=list(emotions_reversed_dict.keys())
def plot_metrics(y_true,ypred,standard_format=True,percentages=True,path="",save_plot=True):
  print(confusion_matrix(y_true, ypred)[:,:])
  print("")
  f1=f1_score(y_true, ypred,average=None)
  f1_w=f1_score(y_true, ypred,average='weighted')
  ac=accuracy_score(y_true, ypred)
  print("accuracy: ",ac)
  print("f1 score:",f1)
  print("f1 score weighted:",f1_w)
  print("---------------------------------------------------------")
  print(path)
  if standard_format:
    cm = confusion_matrix(y_true, ypred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap=plt.cm.Blues,xticks_rotation='vertical',values_format='d')
    if save_plot:
      plt.savefig(path+".png")
    plt.show()
  print("")
  if percentages:
    cm = confusion_matrix(y_true, ypred,normalize='true')
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap=plt.cm.Blues,xticks_rotation='vertical',values_format='.2f')
    plt.show()
    if save_plot:
      plt.savefig(path+"_perctg.png")
    plt.show()
  print("")

def plot_model(history,title):
  plt.figure()
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  # plt.plot(history.history['val_categorical_accuracy'])
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'val'], loc='upper left')

  plt.title("model train and val accuracy from "+title)
  # plt.savefig("Combinedmodel1.png")
  # plt.show()
  
  plt.figure()
  plt.plot(history.history['val_accuracy'])
  plt.title("Validation accuracy " + title)

  plt.figure()
  plt.plot(history.history['val_categorical_accuracy'])
  plt.title("Validation categorical accuracy"+title)

"""## **Load prediction vectors**"""

labels_dirs=["/content/train_video_labels.pkl","/content/val_video_labels.pkl"]

with open(labels_dirs[0], 'rb') as f:
  train_labels = pickle.load(f)

with open(labels_dirs[1], 'rb') as f:
  val_labels = pickle.load(f)

#AUDIO predictions with VGG model trained with Spectrograms converted into images
audio_preds=["/content/vgg_audio_model_preds_train.pkl","/content/vgg_audio_model_preds_test.pkl"]

with open(audio_preds[0], 'rb') as f:
  audio_train_preds = pickle.load(f)

with open(audio_preds[1], 'rb') as f:
  audio_test_preds = pickle.load(f)

#VIDEO predictions with AFEW video images predicted with VGG16 model trained with the SFEW images
# video_preds=["/content/afew_videos_imgs_predictions_m2_tr.pkl","/content/afew_videos_imgs_predictions_m2_val.pkl","/content/afew_videos_imgs_predictions_m2_te.pkl"]

video_preds=["/content/afew_videos_imgs_predictions_dict_m3_tr.pkl","/content/afew_videos_imgs_predictions_dict__bo_m3_te.pkl"]

with open(video_preds[0], 'rb') as f:
  video_train_preds = pickle.load(f)

with open(video_preds[1], 'rb') as f:
  video_test_preds = pickle.load(f)

#VIDEO with SFEW video images predicted with VGG16 model trained with the SFEW images
sfew_preds=["/content/sfew_predictions_tr.pkl","/content/sfew_predictions_te.pkl"]

with open(sfew_preds[0], 'rb') as f:
  sfew_video_train_preds = pickle.load(f)

with open(sfew_preds[1], 'rb') as f:
  sfew_video_test_preds = pickle.load(f)

print(len(train_labels))
print(len(val_labels))
print("_____________")

print(len(audio_train_preds))
print(len(audio_test_preds))
print("_____________")

print(len(video_train_preds))
print(len(video_test_preds))

print("_____________")
print(len(sfew_video_train_preds))
print(len(sfew_video_test_preds))

"""**Evaluamos cada uno de los modelos individuales**

**Figura  17:**  Matriz  de   confusión   obtenida  con   la  VGG16entrenada  con  las  imágenes  de  los  espectrogramas  aplicandofine-tuning a los dos últimos bloques convolucionales


**Figura  13:**  Resultados  para  la  predicción  de  vídeo  utilizandola VGG16 reentrenada con el SFEW
"""

#test 0 obtain accuracy and confussion matrix of each set of predictions:

#Test set audio:
print("audio")
predictions,real_labels=[],[]
for audio_id in audio_test_preds.keys():
  predictions.append(get_emo(audio_test_preds[audio_id][0]))
  real_labels.append(val_labels[audio_id])
  
numeric_real_label=[emotions_reversed_dict[label] for label in real_labels]
numeric_preds=[emotions_reversed_dict[pred] for pred in predictions]

plot_metrics(numeric_real_label,numeric_preds,standard_format=True,percentages=True,path="audio")

print("_______________________________________________")
print("VIDEO:")
#Test set video:

predictions,real_labels=[],[]
for id in video_test_preds.keys():
  predictions.append(get_emo(video_test_preds[id][0]))
  real_labels.append(val_labels[id])
  
numeric_real_label=[emotions_reversed_dict[label] for label in real_labels]
numeric_preds=[emotions_reversed_dict[pred] for pred in predictions]

plot_metrics(numeric_real_label,numeric_preds,standard_format=True,percentages=True,path="video_afew_frames")

print("-------------------------------------------")
print("sfew images:")

predictions,real_labels=[],[]
for id in sfew_video_test_preds.keys():
  predictions.append(get_emo(sfew_video_test_preds[id][0]))
  real_labels.append(val_labels[id])

numeric_real_label=[emotions_reversed_dict[label] for label in real_labels]
numeric_preds=[emotions_reversed_dict[pred] for pred in predictions]

plot_metrics(numeric_real_label,numeric_preds,standard_format=True,percentages=True,path="video_afew_frames")

#Crear los vectores de features encadenando los vectores del audio con los del vídeo

#Extract train features:
#Vector de features de video concatenadas con audio

feature_vectors,feature_labels=[],[]

for id in list(video_train_preds.keys()):
  feature=np.append(video_train_preds[id][0], audio_train_preds[id][0] )
  feature_vectors.append(feature)
  feature_labels.append(emotions_reversed_dict[train_labels[id]])

x_train=feature_vectors
y_train=tf.keras.utils.to_categorical(feature_labels)

#Extract val features:
feature_vectors,feature_labels=[],[]

for id in list(video_test_preds.keys()):
  feature=np.append(video_test_preds[id][0], audio_test_preds[id][0] )
  feature_vectors.append(feature)
  feature_labels.append(emotions_reversed_dict[val_labels[id]])

x_val=feature_vectors
y_val=tf.keras.utils.to_categorical(feature_labels)

"""### **Configuration 1: Average**"""

# feature_labels
#avg acc
(39+27)/2

"""**Figura   19:**   Resultados   de   promediar   las   predicciones   delmodelo del audio y el de vídeo"""

# TEST 0 promig audio i video:
#prediccions video amb frames sfew

audio_preds_sum,video_preds_sum=[],[]
predictions,real_labels,feature_keys=[],[],[]

feature_vectors_dict={}

for id in sfew_video_test_preds.keys():

  #test de que esten normalizados
  audio_preds_sum.append(np.sum(audio_test_preds[id]))
  video_preds_sum.append(np.sum(sfew_video_test_preds[id][0]))

  #media de las predicciones
  avgd_pred=(sfew_video_test_preds[id]+audio_test_preds[id][0])/2
  predictions.append(get_emo(avgd_pred[0]))
  real_labels.append(val_labels[id])
  
print(set(audio_preds_sum))
print(set(video_preds_sum))
print(" ")
plot_metrics(real_labels,predictions,standard_format=True,percentages=True,path="video_afew_frames")
##REVISAR xq las probabilidades del video suman más de 1 !!

# TEST 0 promig audio i video:
#prediccions video amb frames afew
audio_preds_sum,video_preds_sum=[],[]
predictions,real_labels,feature_keys=[],[],[]

feature_vectors_dict={}

for id in video_test_preds.keys():

  #test de que esten normalizados
  audio_preds_sum.append(np.sum(audio_test_preds[id]))
  video_preds_sum.append(np.sum(video_test_preds[id][0]))

  #media de las predicciones
  avgd_pred=(video_test_preds[id]+audio_test_preds[id][0])/2
  predictions.append(get_emo(avgd_pred[0]))
  real_labels.append(val_labels[id])
  
print(set(audio_preds_sum))
print(set(video_preds_sum))
print(" ")
plot_metrics(real_labels,predictions,standard_format=True,percentages=True,path="video_afew_frames")

#Extract train features:
#Vector de features de video concatenadas con audio

feature_vectors,feature_labels=[],[]

for id in list(sfew_video_train_preds.keys()):
  feature=np.append(sfew_video_train_preds[id][0], audio_train_preds[id][0] )
  feature_vectors.append(feature)
  feature_labels.append(emotions_reversed_dict[train_labels[id]])

x_train=feature_vectors
y_train=tf.keras.utils.to_categorical(feature_labels)

#Extract val features:
feature_vectors,feature_labels=[],[]

for id in list(sfew_video_test_preds.keys()):
  feature=np.append(sfew_video_test_preds[id][0], audio_test_preds[id][0] )
  feature_vectors.append(feature)
  feature_labels.append(emotions_reversed_dict[val_labels[id]])

x_test=feature_vectors
y_test=tf.keras.utils.to_categorical(feature_labels)

print(asarray(x_train).shape)
print(asarray(x_test).shape)
print(asarray(y_train).shape)
print(asarray(y_test).shape)

x_train=asarray(x_train)
x_test=asarray(x_test)
y_train=asarray(y_train)
y_test=asarray(y_test)

"""## **Sequential model**"""

def model1():
  #Model 1
  model = Sequential()
  model.add(Dense(units=28,input_dim=14,activation='relu'))
  model.add(Dropout(0.25))
  # model.add(BatchNormalization())
  model.add(Dense(units=N_OUTPUT_LABELS,
                  activation='softmax'))
  model.summary()

  optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)
  model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy', metrics=['categorical_accuracy','accuracy'])
  return model

"""**Figura 20:** Train y test accuracy para el modelo secuencial de la combinación"""

#Model 1
model = Sequential()
model.add(Dense(units=28,input_dim=14,activation='relu'))
model.add(Dropout(0.25))
# model.add(BatchNormalization())
model.add(Dense(units=N_OUTPUT_LABELS,
                activation='softmax'))
model.summary()

optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)
model.compile(optimizer=optimizer,
                loss='categorical_crossentropy', metrics=['categorical_accuracy','accuracy'])

lr=0.001
batch_size=8
epochs=50
# model=model1()
history=model.fit(x_train, y_train,
                  epochs=epochs,
                  validation_data=(x_test, y_test),
                  batch_size=batch_size)

plot_model(history,"test 1 combined model sfew frames")

"""**Figura 21:** Resultados de combinar el audio y el vídeo con elmodelo secuencial"""

predictions=model.predict(x_test)

labels=[key for key in emotions_reversed_dict.keys()]
ypred=[round(list(i).index(np.max(i))) for i in predictions]
y_true=[np.argmax(y) for y in y_test]
plot_metrics(y_true,ypred,standard_format=True,percentages=True,path="modelo1 combinación",save_plot=True)

"""### **SVM**"""

# SVM test:
# Escalar los vectores
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train)
X_test_scaled = scaler.transform(x_val)
y_true_tr=[ get_emo_value(pred) for pred in y_train]
y_true_te=[ get_emo_value(pred) for pred in y_val]

rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_train_scaled, y_true_tr)
rbf_pred = rbf.predict(X_test_scaled)

print("accuracy: ",accuracy_score(y_true_te, rbf_pred))
print(confusion_matrix(y_true_te, rbf_pred))
print("f1: ",f1_score(y_true_te, rbf_pred,average='weighted'))

# Commented out IPython magic to ensure Python compatibility.
# Performing CV to tune parameters for best SVM fit 

from sklearn.svm import SVC
C_range = [1, 10, 100]
gamma_range = [ 1e-1,1,1e2]

params_grid = [{'kernel': ['rbf'], 'gamma': gamma_range,
                     'C': C_range},{'kernel': ['poly'],'degree':[7], 'C': C_range}]


# 
svm_model = GridSearchCV(SVC(), params_grid)
svm_model.fit(X_train_scaled, y_true_tr)

print("The best parameters are %s with a score of %0.2f"
#       % (svm_model.best_params_, svm_model.best_score_))

"""**Figura  22:**  Resultados  de  combinar  el  audio  y  el  vídeo  utili-zando SVM"""

rbf = svm.SVC(kernel='rbf', gamma=0.1, C=1).fit(X_train_scaled, y_true_tr)
rbf_pred = rbf.predict(X_test_scaled)

print("accuracy: ",accuracy_score(y_true_te, rbf_pred))
print(confusion_matrix(y_true_te, rbf_pred))

f1_score(y_true_te, rbf_pred, average='weighted')

plot_metrics(y_true_te,rbf_pred,standard_format=True,percentages=True,path="modelo1 combinación",save_plot=True)



"""### **Test frames afew**

Mismo proceso anterior pero con las predicciones obtenidas con los frames del afew:
"""

# test con el modelo con los frames del afew:

#Extract train features:
#Vector de features de video concatenadas con audio

feature_vectors,feature_labels=[],[]

for id in list(video_train_preds.keys()):
  feature=np.append(video_train_preds[id][0], audio_train_preds[id][0] )
  feature_vectors.append(feature)
  feature_labels.append(emotions_reversed_dict[train_labels[id]])

x_train=feature_vectors
y_train=tf.keras.utils.to_categorical(feature_labels)

#Extract val features:
feature_vectors,feature_labels=[],[]

for id in list(video_test_preds.keys()):
  feature=np.append(video_test_preds[id][0], audio_test_preds[id][0] )
  feature_vectors.append(feature)
  feature_labels.append(emotions_reversed_dict[val_labels[id]])

x_test=feature_vectors
y_test=tf.keras.utils.to_categorical(feature_labels)
print(asarray(x_train).shape)
print(asarray(x_test).shape)
print(asarray(y_train).shape)
print(asarray(y_test).shape)

x_train=asarray(x_train)
x_test=asarray(x_test)
y_train=asarray(y_train)
y_test=asarray(y_test)

# TEST 0 promig audio i video:
#prediccions video amb frames sfew

audio_preds_sum,video_preds_sum=[],[]
predictions,real_labels,feature_keys=[],[],[]

feature_vectors_dict={}

for id in video_test_preds.keys():

  #test de que esten normalizados
  audio_preds_sum.append(np.sum(audio_test_preds[id]))
  video_preds_sum.append(np.sum(video_test_preds[id][0]))

  #media de las predicciones
  avgd_pred=(video_test_preds[id]+audio_test_preds[id][0])/2
  predictions.append(get_emo(avgd_pred[0]))
  real_labels.append(val_labels[id])
  
print(set(audio_preds_sum))
print(set(video_preds_sum))
print(" ")
plot_metrics(real_labels,predictions,standard_format=True,percentages=True,path="video_afew_frames")

lr=0.001
batch_size=8
epochs=50
model=model1()
history=model.fit(x_train, y_train,
                  epochs=epochs,
                  validation_data=(x_test, y_test),
                  batch_size=batch_size)

plot_model(history," combined sequential model1 trained on set 2 (afew frames)")
predictions=model.predict(x_test)

labels=[key for key in emotions_reversed_dict.keys()]
ypred=[round(list(i).index(np.max(i))) for i in predictions]
y_true=[np.argmax(y) for y in y_test]
plot_metrics(y_true,ypred,standard_format=True,percentages=True,path="modelo1 combinación",save_plot=True)

plot_model(history," combined sequential model1 trained on set 2 (afew frames)")

lr=0.001
batch_size=8
epochs=50
model=model1()
history=model.fit(x_train, y_train,
                  epochs=epochs,
                  validation_data=(x_test, y_test),
                  batch_size=batch_size)

plot_model(history," combined sequential model1 trained on set 2 (afew frames)")
predictions=model.predict(x_test)

labels=[key for key in emotions_reversed_dict.keys()]
ypred=[round(list(i).index(np.max(i))) for i in predictions]
y_true=[np.argmax(y) for y in y_test]
plot_metrics(y_true,ypred,standard_format=True,percentages=True,path="modelo1 combinación",save_plot=True)

# SVM test:

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train)
X_test_scaled = scaler.transform(x_val)
y_true_tr=[ get_emo_value(pred) for pred in y_train]
y_true_te=[ get_emo_value(pred) for pred in y_val]

rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_train_scaled, y_true_tr)
rbf_pred = rbf.predict(X_test_scaled)

print("accuracy: ",accuracy_score(y_true_te, rbf_pred))
print(confusion_matrix(y_true_te, rbf_pred))
print("f1: ",f1_score(y_true_te, rbf_pred,average='weighted'))

# Commented out IPython magic to ensure Python compatibility.
# Performing CV to tune parameters for best SVM fit 

from sklearn.svm import SVC
C_range = [1, 10, 100]
gamma_range = [ 1e-1,1,1e2]

params_grid = [{'kernel': ['rbf'], 'gamma': gamma_range,
                     'C': C_range},{'kernel': ['poly'],'degree':[7], 'C': C_range}]


# 
svm_model = GridSearchCV(SVC(), params_grid)
svm_model.fit(X_train_scaled, y_true_tr)

print("The best parameters are %s with a score of %0.2f"
#       % (svm_model.best_params_, svm_model.best_score_))

rbf = svm.SVC(kernel='rbf', gamma=0.1, C=1).fit(X_train_scaled, y_true_tr)
rbf_pred = rbf.predict(X_test_scaled)

print("accuracy: ",accuracy_score(y_true_te, rbf_pred))
print(confusion_matrix(y_true_te, rbf_pred))

f1_score(y_true_te, rbf_pred, average='weighted')

plot_metrics(y_true_te,rbf_pred,standard_format=True,percentages=True,path="modelo1 combinación",save_plot=True)

